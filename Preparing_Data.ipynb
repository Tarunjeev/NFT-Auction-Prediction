{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preparing_Data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**INSTALLING DEPENDENCIES**"
      ],
      "metadata": {
        "id": "XCoAIYtyd4fR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnp36fnOc_M3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import  datetime\n",
        "from datetime import datetime, timedelta\n",
        "import seaborn as sns\n",
        "\n",
        "# ! pip install mitoinstaller\n",
        "# ! python -m mitoinstaller install\n",
        "# ! pip install openpyxl\n",
        "! pip install xgboost\n",
        "# ! pip install knnmv\n",
        "# ! pip install lightgbm\n",
        "import xgboost as xg\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# import lightgbm as lgb\n",
        "# from knnmv.impute import KNNMVImputer\n",
        "# from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tools for processing data\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.decomposition import TruncatedSVD, PCA\n",
        "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, recall_score, classification_report, confusion_matrix, make_scorer, adjusted_rand_score, silhouette_score, homogeneity_score, normalized_mutual_info_score\n",
        "# Classifiers, supervised and unsupervised\n",
        "from sklearn import ensemble\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from time import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sklearn\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import imageio\n",
        "import re\n",
        "from collections import Counter\n",
        "import spacy\n",
        "from time import time\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "kVtXLsOcdwKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OBTAINING DATA**"
      ],
      "metadata": {
        "id": "FfMJsunhevLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_data = pd.read_csv('../input/stat440-21-project2/pred.csv')\n",
        "test_data = pd.read_csv('../input/stat440-21-project2/Xte.csv')\n",
        "train_data = pd.read_csv('../input/stat440-21-project2/XYtr.csv')\n",
        "XYtr = pd.read_csv('../input/stat440-21-project2/XYtr.csv')\n",
        "Xte  = pd.read_csv('../input/stat440-21-project2/Xte.csv')\n"
      ],
      "metadata": {
        "id": "GSkAVqXdeo5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREPARIG DATA**"
      ],
      "metadata": {
        "id": "rjOrMxp7fkZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name = list([\"XYtr\",\"Xte\"])\n",
        "j = 0\n",
        "for df in [XYtr,Xte]:\n",
        "    fp = open(str(name[j]) + '_fi.csv', 'w')\n",
        "    fp.write('id,fi1,fi2,fi3,fi4,fi5,fi6,fi7\\n')\n",
        "    for i in range(df.shape[0]):\n",
        "        id = df.loc[i,'id']\n",
        "        f = df.loc[i,'id'] + df.loc[i, 'ext']\n",
        "        try:\n",
        "            pic = imageio.imread('../input/stat440-21-project2/images/images/' + f)\n",
        "            fi1 = pic.shape[0]\n",
        "            fi2 = pic.shape[1]\n",
        "            fi3 = np.mean(pic[:,:,:])\n",
        "            fi4 = pic.min()\n",
        "            fi5 = np.mean(pic[:,:,0])\n",
        "            fi6 = np.mean(pic[:,:,1])\n",
        "            fi7 = np.mean(pic[:,:,2])\n",
        "    \n",
        "        except:\n",
        "            fi1 = np.nan\n",
        "            fi2 = np.nan\n",
        "            fi3 = np.nan\n",
        "            fi4 = np.nan\n",
        "            fi5 = np.nan\n",
        "            fi6 = np.nan\n",
        "            fi7 = np.nan\n",
        "            pass\n",
        "    \n",
        "        fp.write('%s,%f,%f,%f,%f,%f,%f,%f\\n' % (id, fi1,fi2,fi3,fi4,fi5,fi6,fi7))\n",
        "        fp.close()  \n",
        "        j = j + 1\n",
        "XYtr_fi = pd.read_csv('./XYtr_fi.csv')\n",
        "XYtr_fi1 = XYtr_fi.drop(['id'], axis = 1).copy()\n",
        "XYtr_fi1 = XYtr_fi1.fillna(0)\n",
        "Xte_fi = pd.read_csv('./Xte_fi.csv')\n",
        "Xte_fi1 = Xte_fi.drop(['id'], axis = 1).copy()\n",
        "Xte_fi1= Xte_fi1.fillna(0)\n",
        "train_data = pd.concat([XYtr, XYtr_fi1], axis =1)\n",
        "test_data = pd.concat([Xte, Xte_fi1], axis =1)"
      ],
      "metadata": {
        "id": "8Jn4SBX5e5Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "# Make corpus and vocab\n",
        "\n",
        "K = 10\n",
        "XYtr['description'] = XYtr['description'].fillna(\"NAN\")\n",
        "Xte['description'] = Xte['description'].fillna(\"NAN\")\n",
        "corpus = list(XYtr['description'])+list(Xte['description'])\n",
        "vectorizer = CountVectorizer()\n",
        "corpus = vectorizer.fit_transform(corpus)\n",
        "lda = LatentDirichletAllocation(n_components = K)\n",
        "lda.fit(corpus)\n",
        "\n",
        "topics = lda.transform(corpus)\n",
        "N = XYtr.shape[0]\n",
        "\n",
        "fp = open('XYtr_ft.csv', 'w')\n",
        "fp.write('id')\n",
        "for k in range(K):\n",
        "    fp.write(',FT%04d' % k)\n",
        "\n",
        "fp.write('\\n')\n",
        "for i in range(N):\n",
        "    id = XYtr.loc[i,'id']\n",
        "    fp.write('%s' % id)\n",
        "    for k in range(K):\n",
        "        fp.write(',%f' % topics[i, k])\n",
        "    \n",
        "    fp.write('\\n')\n",
        "\n",
        "fp.close()    \n",
        "fp = open('Xte_ft.csv', 'w')\n",
        "fp.write('id')\n",
        "for k in range(K):\n",
        "    fp.write(',FT%04d' % k)\n",
        "\n",
        "fp.write('\\n')\n",
        "for i in range(N):\n",
        "    id = Xte.loc[i,'id']\n",
        "    fp.write('%s' % id)\n",
        "    for k in range(K):\n",
        "        fp.write(',%f' % topics[i + N, k])\n",
        "    \n",
        "    fp.write('\\n')\n",
        "\n",
        "fp.close()    \n",
        "\n",
        "XYtr_ft = pd.read_csv('XYtr_ft.csv')\n",
        "XYtr_ft1 = XYtr_ft.drop(['id'], axis = 1).copy()\n",
        "XYtr_ft1 = XYtr_ft1.fillna(0)\n",
        "\n",
        "Xte_ft = pd.read_csv('Xte_ft.csv')\n",
        "Xte_ft1 = Xte_ft.drop(['id'], axis = 1).copy()\n",
        "Xte_ft1= Xte_ft1.fillna(0)\n",
        "\n",
        "train_data = pd.concat([train_data, XYtr_ft1], axis =1)\n",
        "test_data = pd.concat([test_data, Xte_ft1], axis =1)\n",
        "# combining test and train data for imputation\n",
        "combined_data = pd.concat([train_data, test_data], ignore_index=True)\n",
        "combined_data = combined_data.drop(['total'], axis = 1)"
      ],
      "metadata": {
        "id": "qEi57vR9fx1a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}