{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exploring_Data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Data Exploration**"
      ],
      "metadata": {
        "id": "6tt10CxMgaLi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4YCg8_ogVFW"
      },
      "outputs": [],
      "source": [
        "# Prcentage of data missing in combined dataset\n",
        "all_data_na = (combined_data.isnull().sum() / len(combined_data)) * 100\n",
        "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\n",
        "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
        "missing_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Skewness of the combined_data set\n",
        "combined_data.agg(['skew']).transpose()"
      ],
      "metadata": {
        "id": "lnmM5KP4gkza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trying to deacrease the skew\n",
        "np.log1p(combined_data[\"X.sales\"]).agg(['skew']).transpose()"
      ],
      "metadata": {
        "id": "d1bA5sLBglzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.info()"
      ],
      "metadata": {
        "id": "YXPQhAKdglxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.nunique()"
      ],
      "metadata": {
        "id": "FZqQJx3DglvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in [\"symbol\", \"description\", \"fee1\", \"fee2\", \"version\"]:\n",
        "  print(train_data[x].unique())"
      ],
      "metadata": {
        "id": "GoIdLQwJgls7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import skew, mode\n",
        "# Missing value filling for fee1\n",
        "agg_func_stats = {'fee1': [pd.Series.count, mode, pd.Series.mode]}\n",
        "train_data.groupby(['cdate']).agg(agg_func_stats).reset_index()[0:40]"
      ],
      "metadata": {
        "id": "LHoF6PJ2glq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing value filling for fee2\n",
        "agg_func_stats = {'version': [pd.Series.count, mode, pd.Series.mode]}\n",
        "combined_data[combined_data[\"symbol\"].isnull() == False]"
      ],
      "metadata": {
        "id": "9-bfuuD8gloS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lakshay_imputer(data,col_change):\n",
        "    # new cases imputation using mode\n",
        "    temp_impute = data[data[col_change].isnull()].index\n",
        "    agg_func_stats = {col_change: [pd.Series.count, pd.Series.mode]}\n",
        "    tmp = data.groupby(['cdate']).agg(agg_func_stats).reset_index()\n",
        "    \n",
        "    # for i in temp_impute: \n",
        "    #       print(data.iloc[i][col_change])\n",
        "  \n",
        "    for i in temp_impute: \n",
        "        if (tmp[tmp['cdate'] == data.iloc[i]['cdate']][(col_change,'count')].iloc[0] != 0.0):\n",
        "          if (isinstance(tmp[tmp['cdate'] == data.iloc[i]['cdate']][[(col_change,'mode')]].iloc[0].iloc[0], float)):\n",
        "            data.at[i, col_change ] = tmp[tmp['cdate'] == data.iloc[i]['cdate']][[(col_change,'mode')]].iloc[0].iloc[0]\n",
        "          else:\n",
        "            data.at[i, col_change ] = tmp[tmp['cdate'] == data.iloc[i]['cdate']][[(col_change,'mode')]].iloc[0].iloc[0][0]\n",
        "        \n",
        "    # for i in temp_impute: \n",
        "    #     print(data.iloc[i][col_change])\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "i1oGTC4NglmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = combined_data.copy()\n",
        "col_change = 'fee2'\n",
        "data = lakshay_imputer(data,col_change)\n",
        "\n",
        "\n",
        "col_change = 'fee1'\n",
        "data = lakshay_imputer(data,col_change)\n",
        "# Prcentage of data missing after imputation\n",
        "all_data_na = (data.isnull().sum() / len(data)) * 100\n",
        "all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\n",
        "missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n",
        "missing_data"
      ],
      "metadata": {
        "id": "0B-U2fQfgljq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = combined_data.copy()\n",
        "# stop words\n",
        "# removing words with count = 1 and the top N words\n",
        "stry = list()\n",
        "for mystring in data1['description'].fillna(\"\"):\n",
        "  result = mystring.split()\n",
        "  stry = stry + result\n",
        "\n",
        "from collections import Counter\n",
        "final = Counter(stry)\n",
        "df = pd.DataFrame.from_dict(final, orient='index').reset_index().rename(columns={'index':'word', 0:'count'})\n",
        "\n",
        "# Get names of indexes for which column count has value 1\n",
        "indexNames = df[ df['count'] == 1 ].index\n",
        "stopwords = df.iloc[indexNames]\n",
        "print(len(stopwords['word'].to_list()))\n",
        "# Delete these row indexes from dataFrame\n",
        "df.drop(indexNames , inplace=True)\n",
        "\n",
        "df.sort_values(by='count', ascending=False).reset_index().drop(['index'], axis = 1)\n",
        "stopwords = stopwords['word'].to_list()"
      ],
      "metadata": {
        "id": "Gk4dX1g8glgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = combined_data.copy().fillna(\"\")\n",
        "df = df[['symbol','description']]\n",
        "# concatenate the string\n",
        "df['description'] = df.groupby(['symbol'])['description'].transform(lambda x : ' '.join(x))\n",
        "  \n",
        "# drop duplicate data\n",
        "df = df.drop_duplicates()   \n",
        "df = df.reset_index().drop(['index'], axis =1)\n",
        "# # show the dataframe\n",
        "# print(df[['symbol','description']])\n",
        "# Idea for future:\n",
        "# combine both datasets to make a more refine voculary\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#get the text column \n",
        "docs=df['description'].tolist()\n",
        "\n",
        "#create a vocabulary of words, \n",
        "#ignore words that appear in 85% of documents, \n",
        "#eliminate stop words\n",
        "cv=CountVectorizer(max_df=0.85,ngram_range=(1, 1),lowercase=False,stop_words=frozenset(stopwords))\n",
        "word_count_vector=cv.fit_transform(docs)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "tfidf_transformer.fit(word_count_vector)"
      ],
      "metadata": {
        "id": "TxPWUu59glcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=False)\n",
        "\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
        "    \n",
        "    #use only topn items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        "\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "    \n",
        "    # word index and corresponding tf-idf score\n",
        "    for idx, score in sorted_items:\n",
        "        \n",
        "        #keep track of feature name and its corresponding score\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        "\n",
        "    #create a tuples of feature,score\n",
        "    #results = zip(feature_vals,score_vals)\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "    \n",
        "    return results"
      ],
      "metadata": {
        "id": "1CJOl_eaglZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you only needs to do this once, this is a mapping of index to \n",
        "feature_names=cv.get_feature_names()\n",
        "\n",
        "# get the document that we want to extract keywords from\n",
        "doc=combined_data['description'].fillna(\"\").tolist()\n",
        "\n",
        "#generate tf-idf for the given document\n",
        "tf_idf_vector=tfidf_transformer.transform(cv.transform(doc))\n",
        "\n",
        "#sort the tf-idf vectors by descending order of scores\n",
        "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "#extract only the top n; n here is 10\n",
        "keywords=extract_topn_from_vector(feature_names,sorted_items,100)\n",
        "\n",
        "# now print the results\n",
        "print(\"\\n=====Doc=====\")\n",
        "#print(doc)\n",
        "print(\"\\n===Keywords===\")\n",
        "print(len(sorted_items))\n",
        "somelist = [x for x in sorted_items if x[1] < 0.01]\n",
        "for x in sorted_items:\n",
        "    if(x[1] < 0.01):\n",
        "        sorted_items.remove(x)\n",
        "  \n",
        "print(len(sorted_items)) \n",
        "print(len(somelist))\n",
        "# for k in keywords:\n",
        "#     print(k,keywords[k])\n",
        "# stopwords = stopwords['word']\n",
        "feature_vals = list()\n",
        "for idx, score in somelist:\n",
        "        feature_vals.append(feature_names[idx])\n",
        "print(len(list(set((stopwords + feature_vals)))))\n",
        "stopwords = stopwords + feature_vals\n",
        "'''\n",
        "Uses of this:\n",
        "Now we can iterate it over every symbol we have to create a refrence and then compare \n",
        "every description that lacks a symbol. Then on the basis of similarity with our known numbers we say if a descriptio belongs to a symbol.\n",
        "'''"
      ],
      "metadata": {
        "id": "wXXVnCeAhhEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = combined_data.copy()\n",
        "df['description'] = df['description'].fillna(\"\")\n",
        "df = df[['symbol','description']]\n",
        "# concatenate the string\n",
        "# df['description'] = df.groupby(['symbol'])['description'].transform(lambda x : ' '.join(x))\n",
        "  \n",
        "# # drop duplicate data\n",
        "# df = df.drop_duplicates()   \n",
        "# df = df.reset_index().drop(['index'], axis =1)\n",
        "# # show the dataframe\n",
        "# print(df[['symbol','description']])\n",
        "# Idea for future:\n",
        "# combine both datasets to make a more refine voculary\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#get the text column \n",
        "docs=df['description'].tolist()\n",
        "\n",
        "#create a vocabulary of words, \n",
        "#ignore words that appear in 85% of documents, \n",
        "#eliminate stop words\n",
        "cv=CountVectorizer(max_df=0.85,ngram_range=(1, 1),lowercase=False,stop_words=frozenset(stopwords))\n",
        "word_count_vector=cv.fit_transform(docs)\n",
        "\n",
        "print(len(list(cv.vocabulary_.keys())))\n",
        "svd = TruncatedSVD(1000)\n",
        "svd.fit(word_count_vector)\n",
        "print(svd.explained_variance_ratio_.sum())\n",
        "\n",
        "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
        "lsa_data = lsa.fit_transform(word_count_vector)\n",
        "print(lsa_data.shape)\n",
        "lsa_data = pd.DataFrame(lsa_data)\n",
        "print(lsa_data.head())\n",
        "\n",
        "#create dataframe\n",
        "cv_dataframe=pd.DataFrame(lsa_data)\n",
        "cv_dataframe['symbol'] = df['symbol']\n",
        "print(cv_dataframe)"
      ],
      "metadata": {
        "id": "srOVk7yChhBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store our results in a DataFrame\n",
        "metrics = ['Algorithm','n_train','Features','ARI','Homogeneity',\n",
        "           'Silhouette','Mutual_Info','Cross_Val','Train_Accuracy',\n",
        "           'Test_Accuracy']\n",
        "performance = pd.DataFrame(columns=metrics)\n",
        "final = cv_dataframe\n",
        "#final = pd.merge(combined_data[combined_data[\"symbol\"].isnull() == False].fillna(\"\")[['symbol']], cv_dataframe, on='symbol')"
      ],
      "metadata": {
        "id": "sHj2o4rxhg-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add image data too\n",
        "X = cv_dataframe[cv_dataframe[\"symbol\"].isnull() == False] \n",
        "X[['fi1', 'fi2', 'fi3', 'fi4', 'fi5', 'fi6', 'fi7', 'FT0000', 'FT0001', 'FT0002', 'FT0003', 'FT0004', 'FT0005', 'FT0006',\n",
        "       'FT0007', 'FT0008', 'FT0009']] = combined_data[['fi1', 'fi2', 'fi3', 'fi4', 'fi5', 'fi6', 'fi7', 'FT0000', 'FT0001', 'FT0002', 'FT0003', 'FT0004', 'FT0005', 'FT0006',\n",
        "       'FT0007', 'FT0008', 'FT0009']]\n",
        "y = X['symbol']\n",
        "X = X.drop(['symbol'],axis = 1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split (X,y,test_size=0.3)"
      ],
      "metadata": {
        "id": "vbX7kcgOhg28"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}